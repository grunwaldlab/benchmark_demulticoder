---
title: "dada2_workflow_rps10"
output: html_document
date: "2024-06-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/benchmark_demulticoder/standard_workflow")

library(phyloseq); packageVersion("phyloseq")
library(ggplot2); packageVersion("ggplot2")
library(dada2); packageVersion("dada2")
library(dada2); packageVersion("Biostrings")
library(ShortRead); packageVersion("ShortRead")
library(parallel); packageVersion("parallel")

set.seed(1)
```

## The DADA2 workflow is similar to what is described in the rps10 tutorial in DADA2 documentation, but since amplicons were pooled, I needed to first demultiplex ITS1 reads within samples, or else ITS1 reads will still be included in all downstream steps.  

### Remove Ns from any reads before beginning DADA2 workflow
```{r remove Ns}
path1 <- "~/benchmark_demulticoder/demulticoder/data"
path <- "~/benchmark_demulticoder/standard_workflow/reads"
dir.create(file.path(path, "prefiltered"), recursive = TRUE, showWarnings = FALSE)

fnFs <- sort(list.files(path1, pattern="_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path1, pattern="_R2.fastq.gz", full.names = TRUE))

fnFs.prefilt <- file.path(path, "prefiltered", basename(fnFs))
fnRs.prefilt <- file.path(path, "prefiltered", basename(fnRs))

out <- filterAndTrim(fnFs, fnFs.prefilt, fnRs, fnRs.prefilt, maxN=0, compress=TRUE, multithread=TRUE)
```

### Demultiplex rps10 reads
```{r demultiplex rps10 reads}
FWD <- "GTTGGTTAGAGYARAAGACT"  
REV <- "ATRYYTAGAAAGAYTYGAACT"

allOrients <- function(primer) {
    require(Biostrings)
    dna <- DNAString(primer)  
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

containsPrimer <- function(primer, read) {
  read <- DNAString(as.character(read))
  any(sapply(primer, function(p) {
    p <- DNAString(as.character(p))
    pattern <- as.character(p)
    pattern <- gsub("R", "[AG]", pattern)
    pattern <- gsub("Y", "[CT]", pattern)
    pattern <- gsub("S", "[GC]", pattern)
    pattern <- gsub("W", "[AT]", pattern)
    pattern <- gsub("K", "[GT]", pattern)
    pattern <- gsub("M", "[AC]", pattern)
    pattern <- gsub("B", "[CGT]", pattern)
    pattern <- gsub("D", "[AGT]", pattern)
    pattern <- gsub("H", "[ACT]", pattern)
    pattern <- gsub("V", "[ACG]", pattern)
    pattern <- gsub("N", "[ACGT]", pattern)
    grepl(pattern, as.character(read), ignore.case = TRUE)
  }))
}

dir.create(file.path(path, "rps10/demultiplexed"), recursive = TRUE, showWarnings = FALSE)

fnFs.demultiplex <- file.path(path, "rps10/demultiplexed", basename(fnFs))
fnRs.demultiplex <- file.path(path, "rps10/demultiplexed", basename(fnRs))

num_cores <- parallel::detectCores() - 1  # Use all but one core

process_file_pair <- function(i) {
  cat("Processing file:", basename(fnFs.prefilt[i]), "\n")
  
  readsF <- readFastq(fnFs.prefilt[i])
  readsR <- readFastq(fnRs.prefilt[i])
  
  cat("Number of reads in forward file:", length(readsF), "\n")
  cat("Number of reads in reverse file:", length(readsR), "\n")
  
  hasrps10F <- parallel::mclapply(sread(readsF), containsPrimer, primer = FWD.orients, mc.cores = 1)
  hasrps10R <- parallel::mclapply(sread(readsR), containsPrimer, primer = REV.orients, mc.cores = 1)
  
  hasrps10F <- unlist(hasrps10F)
  hasrps10R <- unlist(hasrps10R)
  
  cat("Reads with forward primer:", sum(hasrps10F), "\n")
  cat("Reads with reverse primer:", sum(hasrps10R), "\n")
  
  keepReads <- hasrps10F | hasrps10R
  
  cat("Total reads with rps10 primers:", sum(keepReads), "\n")
  
  if (sum(keepReads) == 0) {
    cat("WARNING: No reads with rps10 primers found in this file pair.\n")
    # Print a few reads to check
    cat("Sample of first 5 forward reads:\n")
    for (j in 1:5) {
      cat("Read", j, ":\n")
      cat(as.character(sread(readsF)[j]), "\n")
      cat("Quality scores:\n")
      cat(as.character(quality(quality(readsF)[j])), "\n\n")
    }
    cat("Sample of first 5 reverse reads:\n")
    for (j in 1:5) {
      cat("Read", j, ":\n")
      cat(as.character(sread(readsR)[j]), "\n")
      cat("Quality scores:\n")
      cat(as.character(quality(quality(readsR)[j])), "\n\n")
    }
  } else {
    # Subset the reads
    rps10ReadsF <- readsF[keepReads]
    rps10ReadsR <- readsR[keepReads]
    
    writeFastq(rps10ReadsF, fnFs.demultiplex[i], compress = TRUE)
    writeFastq(rps10ReadsR, fnRs.demultiplex[i], compress = TRUE)
    
    cat("Written to:", fnFs.demultiplex[i], "\n")
    cat("Written to:", fnRs.demultiplex[i], "\n")
  }
  
  cat("\n")
}

mclapply(seq_along(fnFs.prefilt), process_file_pair, mc.cores = num_cores)
```

## Get stats on primer counts before running Cutadapt 
```{r get primer counts}
FWD <- "GTTGGTTAGAGYARAAGACT"  
REV <- "ATRYYTAGAAAGAYTYGAACT"

allOrients <- function(primer) {
    require(Biostrings)
    dna <- DNAString(primer)  
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
}

primerHits <- function(primer, fn) {
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.demultiplex[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.demultiplex[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.demultiplex[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.demultiplex[[1]]))
```

### Run Cutadapt to remove primers
```{r run cutadapt}
cutadapt <- "/usr/bin/cutadapt"
system2(cutadapt, args = "--version") 

path.cut <- file.path(path, "rps10/cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, 
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], 
                             "--minimum-length", 50,
                             fnFs.demultiplex[i], fnRs.demultiplex[i]))}
```

### Get stats on primer counts after running cutadapt 
```{r get post-trim primer counts}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.cut[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.cut[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

### Now we will inspect read quality profiles
``` {r plot qualityp rofiles}
cutFs <- sort(list.files(path.cut, pattern = "_R1.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2.fastq.gz", full.names = TRUE))

get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)

plotQualityProfile(cutFs[1:5])
plotQualityProfile(cutRs[1:5])
```

## Run core filterAndTrim command from DADA2 to remove any poor quality reads
```{r filter and trim reads}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(5, 5), truncQ = 5,
    minLen = 50, maxLen = Inf, rm.phix = TRUE, compress = TRUE, multithread = TRUE) 
head(out)

out
```

### Now we will learn error rates
``` {r learn error rates}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)
```

### Next is the key sample inference step
``` {r sample inference}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE)
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)
```

### We will merge paired reads
``` {r merged paired}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs,  minOverlap = 15, maxMismatch = 2, verbose=TRUE)
```

### We will then construct a sequence table
```{r construct sequence table}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

### Finally, we will remove chimeras
```{r remove chimeras}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
table(nchar(getSequences(seqtab.nochim)))

sample_prefixes <- stringr::str_remove(rownames(seqtab.nochim), "_R1\\.fastq\\.gz$")
rownames(seqtab.nochim) <- sample_prefixes

write.table(seqtab.nochim, file="rps10/outputs/rps10_seqtab_nochim.out",sep="\t",quote=F)
```

### At the end, we will track reads through the full DADA2 pipeline
``` {r plotqualityprofiles}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")

sample_prefixes <- stringr::str_remove(rownames(track), "_R1\\.fastq\\.gz$")
rownames(track) <- sample_prefixes

head(track)
write.table(track, file="rps10/outputs/rps10_trackreads.out",sep="\t",quote=F)
```

### If everything look good, let's assign taxonomy
``` {r assignTaxonomy}
oomycetedb.ref <- "~/benchmark_demulticoder/standard_workflow/rps10/data/rps10_reference_db_unique.fa"  # CHANGE ME to location on your machine
taxa <- assignTaxonomy(seqtab.nochim, oomycetedb.ref, multithread = TRUE, minBoot = 0, outputBootstraps = TRUE, verbose=TRUE, tryRC = FALSE, taxLevels = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species", "Reference"))

taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL

write.table(taxa, file="rps10/outputs/rps10_taxa_finaladj.out",sep="\t",quote=F)
```

### Let's collect info on R configuration and associated package versions that are downloaded
```{r session info}
sessioninfo::session_info()
```